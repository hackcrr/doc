# è·å–å¤‡ä»½åˆ—è¡¨

## ç«¯ç‚¹ä¿¡æ¯

```http
GET /database/{db_name}/backups
Authorization: Bearer your_api_key
```

è·å–æŒ‡å®šæ•°æ®åº“çš„æ‰€æœ‰å¤‡ä»½æ–‡ä»¶åˆ—è¡¨å’Œä¿¡æ¯ã€‚

## æƒé™è¦æ±‚
- `list_backups` æƒé™
- å¯¹ç›®æ ‡æ•°æ®åº“çš„ `read` æƒé™

## è¯·æ±‚

### è¯·æ±‚å¤´
| å¤´éƒ¨ | å€¼ | å¿…å¡« |
|------|-----|------|
| `Authorization` | `Bearer your_api_key` | æ˜¯ |

### è·¯å¾„å‚æ•°
| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `db_name` | string | æ˜¯ | æ•°æ®åº“åç§° |

## å“åº”

### æˆåŠŸå“åº”
**çŠ¶æ€ç :** `200 OK`

```json
{
  "database": "my_database",
  "backup_count": 3,
  "backups": [
    {
      "filename": "admin_my_database_manual_backup_2024_20240120_143000.sql.gz",
      "filepath": "/backups/admin_my_database_manual_backup_2024_20240120_143000.sql.gz",
      "size": 1572864,
      "size_human": "1.50 MB",
      "created_time": "2024-01-20T14:30:00",
      "modified_time": "2024-01-20T14:35:00",
      "compressed": true,
      "backup_type": "manual",
      "timestamp": "20240120_143000"
    },
    {
      "filename": "admin_my_database_auto_backup_20240119_020000.sql",
      "filepath": "/backups/admin_my_database_auto_backup_20240119_020000.sql",
      "size": 2097152,
      "size_human": "2.00 MB",
      "created_time": "2024-01-19T02:00:00",
      "modified_time": "2024-01-19T02:05:00",
      "compressed": false,
      "backup_type": "auto",
      "timestamp": "20240119_020000"
    }
  ]
}
```

### å“åº”å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `database` | string | æ•°æ®åº“åç§° |
| `backup_count` | integer | å¤‡ä»½æ–‡ä»¶æ•°é‡ |
| `backups` | array | å¤‡ä»½æ–‡ä»¶åˆ—è¡¨ |

#### å¤‡ä»½æ–‡ä»¶ä¿¡æ¯
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `filename` | string | å¤‡ä»½æ–‡ä»¶å |
| `filepath` | string | æ–‡ä»¶å®Œæ•´è·¯å¾„ |
| `size` | integer | æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ |
| `size_human` | string | äººç±»å¯è¯»çš„æ–‡ä»¶å¤§å° |
| `created_time` | string | æ–‡ä»¶åˆ›å»ºæ—¶é—´ |
| `modified_time` | string | æ–‡ä»¶ä¿®æ”¹æ—¶é—´ |
| `compressed` | boolean | æ˜¯å¦å‹ç¼© |
| `backup_type` | string | å¤‡ä»½ç±»å‹ |
| `timestamp` | string | æ—¶é—´æˆ³æ ‡è¯† |

## ä½¿ç”¨ç¤ºä¾‹

### cURL ç¤ºä¾‹
```bash
curl -X GET https://dbapi.muzilix.cn/database/my_database/backups \
  -H "Authorization: Bearer your_api_key"
```

### Python ç¤ºä¾‹
```python
def list_database_backups(api_key, db_name):
    """è·å–æ•°æ®åº“å¤‡ä»½åˆ—è¡¨"""
    url = f"https://dbapi.muzilix.cn/database/{db_name}/backups"
    headers = {"Authorization": f"Bearer {api_key}"}
    
    response = requests.get(url, headers=headers)
    return response.json()

def analyze_backup_storage(api_key, db_name):
    """åˆ†æå¤‡ä»½å­˜å‚¨ä½¿ç”¨æƒ…å†µ"""
    backups_data = list_database_backups(api_key, db_name)
    
    if "error" in backups_data:
        print(f"é”™è¯¯: {backups_data['error']}")
        return
    
    print(f"æ•°æ®åº“: {backups_data['database']}")
    print(f"å¤‡ä»½æ•°é‡: {backups_data['backup_count']}")
    
    total_size = 0
    compressed_count = 0
    
    print("\nğŸ“Š å¤‡ä»½æ–‡ä»¶åˆ—è¡¨:")
    for backup in backups_data["backups"]:
        print(f"\nğŸ“ {backup['filename']}")
        print(f"   å¤§å°: {backup['size_human']}")
        print(f"   ç±»å‹: {backup['backup_type']}")
        print(f"   å‹ç¼©: {'æ˜¯' if backup['compressed'] else 'å¦'}")
        print(f"   åˆ›å»ºæ—¶é—´: {backup['created_time']}")
        
        total_size += backup["size"]
        if backup["compressed"]:
            compressed_count += 1
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»å­˜å‚¨ç©ºé—´: {format_size(total_size)}")
    print(f"   å‹ç¼©å¤‡ä»½: {compressed_count}/{backups_data['backup_count']}")

# ä½¿ç”¨ç¤ºä¾‹
backups = list_database_backups("your_api_key", "production_db")
analyze_backup_storage("your_api_key", "production_db")
```

## å¤‡ä»½ç±»å‹è¯´æ˜

### å¤‡ä»½ç±»å‹è¯†åˆ«
ä»æ–‡ä»¶åè§£æå¤‡ä»½ç±»å‹ï¼š
- `manual` - æ‰‹åŠ¨åˆ›å»ºçš„å¤‡ä»½
- `auto` - è‡ªåŠ¨å¤‡ä»½ä»»åŠ¡åˆ›å»º
- `pre_deployment` - éƒ¨ç½²å‰å¤‡ä»½
- å…¶ä»–è‡ªå®šä¹‰ç±»å‹

### æ–‡ä»¶å‘½åæ¨¡å¼
å¤‡ä»½æ–‡ä»¶éµå¾ªç‰¹å®šå‘½åæ¨¡å¼ï¼Œä¾¿äºè¯†åˆ«ï¼š
```
{ç”¨æˆ·å}_{æ•°æ®åº“å}_{ç±»å‹}_{æ—¶é—´æˆ³}.{æ‰©å±•å}
```

## å¤‡ä»½ç®¡ç†

### å­˜å‚¨åˆ†æ
```python
def find_largest_backups(api_key, db_name, limit=5):
    """æŸ¥æ‰¾æœ€å¤§çš„å¤‡ä»½æ–‡ä»¶"""
    backups_data = list_database_backups(api_key, db_name)
    
    if "error" in backups_data:
        return []
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åº
    sorted_backups = sorted(backups_data["backups"], key=lambda x: x["size"], reverse=True)
    return sorted_backups[:limit]

def find_old_backups(api_key, db_name, days_threshold=30):
    """æŸ¥æ‰¾æ—§çš„å¤‡ä»½æ–‡ä»¶"""
    backups_data = list_database_backups(api_key, db_name)
    
    if "error" in backups_data:
        return []
    
    from datetime import datetime, timedelta
    cutoff_date = datetime.now() - timedelta(days=days_threshold)
    
    old_backups = []
    for backup in backups_data["backups"]:
        created_time = datetime.fromisoformat(backup["created_time"].replace('Z', '+00:00'))
        if created_time < cutoff_date:
            old_backups.append(backup)
    
    return old_backups

# ä½¿ç”¨ç¤ºä¾‹
largest_backups = find_largest_backups("your_api_key", "analytics_db", 3)
print("æœ€å¤§çš„å¤‡ä»½æ–‡ä»¶:")
for backup in largest_backups:
    print(f"  {backup['filename']} - {backup['size_human']}")

old_backups = find_old_backups("your_api_key", "analytics_db", 60)
print(f"\nè¶…è¿‡60å¤©çš„æ—§å¤‡ä»½: {len(old_backups)} ä¸ª")
```

### å¤‡ä»½æ¸…ç†å»ºè®®
```python
def suggest_backup_cleanup(api_key, db_name):
    """æä¾›å¤‡ä»½æ¸…ç†å»ºè®®"""
    backups_data = list_database_backups(api_key, db_name)
    
    if "error" in backups_data:
        return
    
    total_count = backups_data["backup_count"]
    max_backups = 10  # å»ºè®®ä¿ç•™çš„æœ€å¤§å¤‡ä»½æ•°
    
    if total_count > max_backups:
        print(f"âš ï¸  å¤‡ä»½æ•°é‡ ({total_count}) è¶…è¿‡å»ºè®®å€¼ ({max_backups})")
        
        # æŒ‰æ—¶é—´æ’åºï¼Œå»ºè®®åˆ é™¤æœ€æ—§çš„
        sorted_backups = sorted(backups_data["backups"], key=lambda x: x["created_time"])
        old_backups = sorted_backups[:total_count - max_backups]
        
        print("å»ºè®®åˆ é™¤çš„æ—§å¤‡ä»½:")
        for backup in old_backups:
            print(f"  - {backup['filename']} ({backup['created_time']})")

# ä½¿ç”¨ç¤ºä¾‹
suggest_backup_cleanup("your_api_key", "production_db")
```

## æ–‡ä»¶ä¿¡æ¯

### æ—¶é—´ä¿¡æ¯
- `created_time`: æ–‡ä»¶åˆ›å»ºæ—¶é—´ï¼ˆISO 8601 æ ¼å¼ï¼‰
- `modified_time`: æœ€åä¿®æ”¹æ—¶é—´
- `timestamp`: ä»æ–‡ä»¶åæå–çš„æ—¶é—´æˆ³æ ‡è¯†

### å¤§å°ä¿¡æ¯
- `size`: åŸå§‹å­—èŠ‚æ•°
- `size_human`: æ ¼å¼åŒ–çš„å¤§å°ï¼ˆKB, MB, GBï¼‰

## ç›¸å…³é“¾æ¥

- [å¤‡ä»½æ¢å¤æ€»è§ˆ](../backup-recovery/index.md)
- [åˆ›å»ºå¤‡ä»½](backup-database.md)
- [ä¸‹è½½å¤‡ä»½](download-backup.md)
- [åˆ é™¤å¤‡ä»½](delete-backup.md)
- [å¤‡ä»½çŠ¶æ€](backup-status.md)