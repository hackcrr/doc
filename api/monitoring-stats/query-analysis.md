# æŸ¥è¯¢åˆ†æç»Ÿè®¡

## ç«¯ç‚¹ä¿¡æ¯

```http
GET /stats/query-analysis
Authorization: Bearer your_api_key
```

è·å–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ç›‘æ§å’Œæ…¢æŸ¥è¯¢åˆ†æä¿¡æ¯ã€‚

## æƒé™è¦æ±‚
- `monitoring` æƒé™
- å¯¹ç›®æ ‡æ•°æ®åº“çš„ `read` æƒé™

## è¯·æ±‚

### è¯·æ±‚å¤´
| å¤´éƒ¨ | å€¼ | å¿…å¡« |
|------|-----|------|
| `Authorization` | `Bearer your_api_key` | æ˜¯ |

### æŸ¥è¯¢å‚æ•°
| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|---------|------|
| `db_name` | string | å¦ | ç©º | æŒ‡å®šæ•°æ®åº“åç§°ï¼Œè¿‡æ»¤ç‰¹å®šæ•°æ®åº“çš„æŸ¥è¯¢ |
| `limit` | integer | å¦ | 10 | è¿”å›çš„æ…¢æŸ¥è¯¢æ•°é‡é™åˆ¶ |

## å“åº”

### æˆåŠŸå“åº”

**çŠ¶æ€ç :** `200 OK`

```json
{
  "timestamp": "2024-01-20T15:30:00",
  "query_stats": {
    "total_queries": 15000,
    "slow_queries": 23,
    "queries_per_second": 45.7,
    "select_queries": 12000,
    "insert_queries": 1500,
    "update_queries": 800,
    "delete_queries": 700
  },
  "slow_queries": [
    {
      "database": "users_db",
      "query_time": 5.234,
      "lock_time": 0.123,
      "rows_sent": 1000,
      "rows_examined": 50000,
      "sql_text": "SELECT * FROM large_table WHERE status = 'active'...",
      "start_time": "2024-01-20T14:25:00"
    }
  ],
  "connection_stats": {
    "total_connections": 15,
    "active_connections": 3,
    "max_used_connections": 25
  },
  "active_connections": [
    {
      "id": 123,
      "user": "api_user",
      "host": "192.168.1.100:54321",
      "db": "users_db",
      "command": "Query",
      "time": 5,
      "state": "Sending data",
      "info": "SELECT * FROM users WHERE age > 30"
    }
  ]
}
```

### æ…¢æŸ¥è¯¢æ—¥å¿—æœªå¯ç”¨å“åº”

**çŠ¶æ€ç :** `200 OK`

```json
{
  "timestamp": "2024-01-20T15:30:00",
  "slow_log_available": false,
  "slow_log_error": "(1054, \"Unknown column 'sql_text' in 'field list'\")",
  "active_connections": [...],
  "connection_stats": {...}
}
```

### å“åº”å­—æ®µè¯´æ˜

#### åŸºæœ¬ä¿¡æ¯
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `timestamp` | string | ç»Ÿè®¡ç”Ÿæˆæ—¶é—´æˆ³ |
| `slow_log_available` | boolean | æ…¢æŸ¥è¯¢æ—¥å¿—æ˜¯å¦å¯ç”¨ |
| `slow_log_error` | string | æ…¢æŸ¥è¯¢æ—¥å¿—é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœä¸å¯ç”¨ï¼‰ |

#### æŸ¥è¯¢ç»Ÿè®¡
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `total_queries` | integer | æ€»æŸ¥è¯¢æ¬¡æ•° |
| `slow_queries` | integer | æ…¢æŸ¥è¯¢æ¬¡æ•° |
| `queries_per_second` | number | æ¯ç§’æŸ¥è¯¢æ•° |
| `select_queries` | integer | SELECT æŸ¥è¯¢æ¬¡æ•° |
| `insert_queries` | integer | INSERT æŸ¥è¯¢æ¬¡æ•° |
| `update_queries` | integer | UPDATE æŸ¥è¯¢æ¬¡æ•° |
| `delete_queries` | integer | DELETE æŸ¥è¯¢æ¬¡æ•° |

#### æ…¢æŸ¥è¯¢è¯¦æƒ…
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `database` | string | æŸ¥è¯¢æ‰€åœ¨çš„æ•°æ®åº“åç§° |
| `query_time` | number | æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ |
| `lock_time` | number | é”ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ |
| `rows_sent` | integer | è¿”å›ç»™å®¢æˆ·ç«¯çš„è¡Œæ•° |
| `rows_examined` | integer | æœåŠ¡å™¨æ‰«æçš„è¡Œæ•° |
| `sql_text` | string | SQLæŸ¥è¯¢è¯­å¥ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰ |
| `start_time` | string | æŸ¥è¯¢å¼€å§‹æ—¶é—´æˆ³ |

#### è¿æ¥ç»Ÿè®¡
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `total_connections` | integer | å½“å‰æ€»è¿æ¥æ•° |
| `active_connections` | integer | æ´»è·ƒè¿æ¥æ•° |
| `max_used_connections` | integer | å†å²æœ€å¤§è¿æ¥æ•° |

#### æ´»è·ƒè¿æ¥è¯¦æƒ…
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | integer | è¿æ¥ID |
| `user` | string | æ•°æ®åº“ç”¨æˆ·å |
| `host` | string | å®¢æˆ·ç«¯ä¸»æœºåœ°å€ |
| `db` | string | å½“å‰ä½¿ç”¨çš„æ•°æ®åº“ |
| `command` | string | æ‰§è¡Œçš„å‘½ä»¤ç±»å‹ |
| `time` | integer | å‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ |
| `state` | string | è¿æ¥å½“å‰çŠ¶æ€ |
| `info` | string | æ­£åœ¨æ‰§è¡Œçš„SQLè¯­å¥ |

## ä½¿ç”¨ç¤ºä¾‹

### cURL ç¤ºä¾‹

**è·å–æ‰€æœ‰æ•°æ®åº“æŸ¥è¯¢åˆ†æ:**
```bash
curl -X GET "https://dbapi.muzilix.cn/stats/query-analysis" \
  -H "Authorization: Bearer your_api_key"
```

**è·å–ç‰¹å®šæ•°æ®åº“æŸ¥è¯¢åˆ†æ:**
```bash
curl -X GET "https://dbapi.muzilix.cn/stats/query-analysis?db_name=users_db&limit=20" \
  -H "Authorization: Bearer your_api_key"
```

### Python ç¤ºä¾‹

```python
import requests
import time
from datetime import datetime

def get_query_analysis(api_key, db_name=None, limit=10):
    """è·å–æŸ¥è¯¢åˆ†æç»Ÿè®¡ä¿¡æ¯"""
    url = "https://dbapi.muzilix.cn/stats/query-analysis"
    params = {}
    
    if db_name:
        params['db_name'] = db_name
    if limit:
        params['limit'] = limit
    
    headers = {"Authorization": f"Bearer {api_key}"}
    
    response = requests.get(url, params=params, headers=headers)
    return response.json()

def analyze_slow_queries(api_key, db_name=None):
    """åˆ†ææ…¢æŸ¥è¯¢æ€§èƒ½"""
    data = get_query_analysis(api_key, db_name, limit=50)
    
    if not data.get('success', True):
        print(f"é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return
    
    print("ğŸ“Š æŸ¥è¯¢æ€§èƒ½åˆ†ææŠ¥å‘Š")
    print(f"   ç”Ÿæˆæ—¶é—´: {data['timestamp']}")
    
    # æŸ¥è¯¢ç»Ÿè®¡
    query_stats = data.get('query_stats', {})
    if query_stats:
        print(f"\nğŸ“ˆ æŸ¥è¯¢ç»Ÿè®¡:")
        print(f"   æ€»æŸ¥è¯¢æ¬¡æ•°: {query_stats.get('total_queries', 0):,}")
        print(f"   æ…¢æŸ¥è¯¢æ¬¡æ•°: {query_stats.get('slow_queries', 0):,}")
        print(f"   æ¯ç§’æŸ¥è¯¢æ•°: {query_stats.get('queries_per_second', 0):.1f}")
        print(f"   SELECT æŸ¥è¯¢: {query_stats.get('select_queries', 0):,}")
        print(f"   INSERT æŸ¥è¯¢: {query_stats.get('insert_queries', 0):,}")
        print(f"   UPDATE æŸ¥è¯¢: {query_stats.get('update_queries', 0):,}")
        print(f"   DELETE æŸ¥è¯¢: {query_stats.get('delete_queries', 0):,}")
    
    # æ…¢æŸ¥è¯¢åˆ†æ
    slow_queries = data.get('slow_queries', [])
    if slow_queries:
        print(f"\nâš ï¸  æ…¢æŸ¥è¯¢åˆ†æ (å…± {len(slow_queries)} ä¸ª):")
        for i, query in enumerate(slow_queries, 1):
            print(f"   {i}. æ•°æ®åº“: {query['database']}")
            print(f"      æ‰§è¡Œæ—¶é—´: {query['query_time']:.3f}s")
            print(f"      é”ç­‰å¾…: {query['lock_time']:.3f}s")
            print(f"      æ‰«æ/è¿”å›: {query['rows_examined']:,}/{query['rows_sent']:,}")
            print(f"      SQL: {query['sql_text'][:100]}...")
            print(f"      æ—¶é—´: {query['start_time']}")
            print()
    else:
        print("\nâœ… æœªå‘ç°æ…¢æŸ¥è¯¢")
    
    # è¿æ¥ç»Ÿè®¡
    conn_stats = data.get('connection_stats', {})
    if conn_stats:
        print(f"\nğŸ”— è¿æ¥ç»Ÿè®¡:")
        print(f"   æ€»è¿æ¥æ•°: {conn_stats.get('total_connections', 0)}")
        print(f"   æ´»è·ƒè¿æ¥: {conn_stats.get('active_connections', 0)}")
        print(f"   æœ€å¤§è¿æ¥: {conn_stats.get('max_used_connections', 0)}")
    
    # æ´»è·ƒè¿æ¥
    active_conns = data.get('active_connections', [])
    if active_conns:
        print(f"\nğŸ”„ æ´»è·ƒè¿æ¥ (å…± {len(active_conns)} ä¸ª):")
        for conn in active_conns:
            if conn['time'] > 10:  # é•¿æ—¶é—´è¿è¡Œçš„è¿æ¥
                print(f"   âš ï¸ è¿æ¥ {conn['id']} è¿è¡Œ {conn['time']}s: {conn['info']}")

def monitor_query_performance(api_key, db_name, threshold_seconds=1.0):
    """ç›‘æ§æŸ¥è¯¢æ€§èƒ½å¹¶è®¾ç½®å‘Šè­¦"""
    data = get_query_analysis(api_key, db_name)
    
    slow_queries = data.get('slow_queries', [])
    critical_slow_queries = [
        q for q in slow_queries 
        if q['query_time'] > threshold_seconds
    ]
    
    if critical_slow_queries:
        print(f"ğŸš¨ æ€§èƒ½å‘Šè­¦: å‘ç° {len(critical_slow_queries)} ä¸ªè¶…è¿‡ {threshold_seconds}ç§’ çš„æ…¢æŸ¥è¯¢")
        for query in critical_slow_queries:
            print(f"   - {query['query_time']:.2f}s: {query['sql_text'][:80]}...")
    
    # è¿æ¥æ± ä½¿ç”¨ç‡å‘Šè­¦
    conn_stats = data.get('connection_stats', {})
    total_conns = conn_stats.get('total_connections', 0)
    active_conns = conn_stats.get('active_connections', 0)
    
    if total_conns > 0 and active_conns / total_conns > 0.8:
        print(f"ğŸš¨ è¿æ¥æ± å‘Šè­¦: ä½¿ç”¨ç‡ {active_conns/total_conns:.1%}")

def performance_trend_analysis(api_key, db_name, interval_minutes=5):
    """æ€§èƒ½è¶‹åŠ¿åˆ†æ"""
    print("ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æä¸­...")
    
    # æ¨¡æ‹Ÿå¤šæ¬¡é‡‡æ ·åˆ†æè¶‹åŠ¿
    samples = []
    for i in range(6):  # é‡‡æ ·6æ¬¡
        data = get_query_analysis(api_key, db_name)
        
        query_stats = data.get('query_stats', {})
        slow_count = len(data.get('slow_queries', []))
        active_conns = data.get('connection_stats', {}).get('active_connections', 0)
        
        samples.append({
            'timestamp': data['timestamp'],
            'slow_queries': slow_count,
            'active_connections': active_conns,
            'qps': query_stats.get('queries_per_second', 0)
        })
        
        if i < 5:  # ä¸æ˜¯æœ€åä¸€æ¬¡é‡‡æ ·
            time.sleep(interval_minutes * 60)
    
    # åˆ†æè¶‹åŠ¿
    print("\nğŸ“Š è¶‹åŠ¿åˆ†æç»“æœ:")
    first = samples[0]
    last = samples[-1]
    
    slow_change = last['slow_queries'] - first['slow_queries']
    conn_change = last['active_connections'] - first['active_connections']
    qps_change = last['qps'] - first['qps']
    
    print(f"   æ…¢æŸ¥è¯¢å˜åŒ–: {slow_change:+d}")
    print(f"   æ´»è·ƒè¿æ¥å˜åŒ–: {conn_change:+d}")
    print(f"   QPS å˜åŒ–: {qps_change:+.1f}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    API_KEY = "your_api_key"
    
    # åŸºæœ¬æŸ¥è¯¢åˆ†æ
    analyze_slow_queries(API_KEY, "production_db")
    
    # æ€§èƒ½ç›‘æ§
    monitor_query_performance(API_KEY, "production_db", threshold_seconds=2.0)
    
    # è¶‹åŠ¿åˆ†æï¼ˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
    # performance_trend_analysis(API_KEY, "production_db")
```

## ç›‘æ§æŒ‡æ ‡è¯´æ˜

### æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡
- **æ…¢æŸ¥è¯¢é˜ˆå€¼**: é»˜è®¤è¶…è¿‡10ç§’çš„æŸ¥è¯¢è¢«è®¤ä¸ºæ˜¯æ…¢æŸ¥è¯¢
- **æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ**: åˆ†æå„ç±»æŸ¥è¯¢çš„æ¯”ä¾‹ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
- **QPS (Queries Per Second)**: æ¯ç§’æŸ¥è¯¢æ•°ï¼Œåæ˜ æ•°æ®åº“è´Ÿè½½

### è¿æ¥æ± æŒ‡æ ‡
- **æ´»è·ƒè¿æ¥æ•°**: å½“å‰æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢çš„è¿æ¥æ•°é‡
- **è¿æ¥ä½¿ç”¨ç‡**: æ´»è·ƒè¿æ¥å æ€»è¿æ¥æ•°çš„æ¯”ä¾‹
- **æœ€å¤§è¿æ¥æ•°**: å†å²æœ€é«˜è¿æ¥ä½¿ç”¨é‡

### æ…¢æŸ¥è¯¢åˆ†æè¦ç‚¹
- **æ‰§è¡Œæ—¶é—´**: è¯†åˆ«æœ€è€—æ—¶çš„æŸ¥è¯¢
- **é”ç­‰å¾…æ—¶é—´**: åˆ†æå¹¶å‘æ€§èƒ½é—®é¢˜
- **æ‰«æè¡Œæ•°**: è¯†åˆ«å…¨è¡¨æ‰«æç­‰ä½æ•ˆæŸ¥è¯¢
- **è¿”å›è¡Œæ•°**: åˆ†ææŸ¥è¯¢æ•ˆç‡

## æœ€ä½³å®è·µ

### 1. å®šæœŸç›‘æ§
```python
def setup_performance_monitoring(api_key, db_name):
    """è®¾ç½®æ€§èƒ½ç›‘æ§ä»»åŠ¡"""
    import schedule
    
    def daily_performance_check():
        analyze_slow_queries(api_key, db_name)
        monitor_query_performance(api_key, db_name)
    
    # æ¯å¤©ä¸Šåˆ9ç‚¹æ‰§è¡Œæ€§èƒ½æ£€æŸ¥
    schedule.every().day.at("09:00").do(daily_performance_check)
    
    # æ¯å°æ—¶æ‰§è¡Œå¿«é€Ÿæ£€æŸ¥
    schedule.every().hour.do(
        lambda: monitor_query_performance(api_key, db_name, 5.0)
    )
```

### 2. æ€§èƒ½ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
```python
def generate_optimization_suggestions(api_key, db_name):
    """ç”ŸæˆæŸ¥è¯¢ä¼˜åŒ–å»ºè®®"""
    data = get_query_analysis(api_key, db_name)
    suggestions = []
    
    for query in data.get('slow_queries', []):
        # åˆ†ææ‰«ææ•ˆç‡
        scan_efficiency = query['rows_sent'] / query['rows_examined'] if query['rows_examined'] > 0 else 0
        
        if scan_efficiency < 0.01:
            suggestions.append(
                f"æŸ¥è¯¢æ‰«ææ•ˆç‡ä½ ({scan_efficiency:.2%})ï¼Œè€ƒè™‘æ·»åŠ ç´¢å¼•: {query['sql_text'][:50]}..."
            )
        
        if query['lock_time'] > 0.1:
            suggestions.append(
                f"æŸ¥è¯¢é”ç­‰å¾…æ—¶é—´è¾ƒé•¿ ({query['lock_time']:.3f}s)ï¼Œè€ƒè™‘ä¼˜åŒ–äº‹åŠ¡: {query['sql_text'][:50]}..."
            )
    
    return suggestions
```

### 3. å®¹é‡è§„åˆ’
```python
def query_capacity_analysis(api_key, db_name):
    """æŸ¥è¯¢å®¹é‡åˆ†æ"""
    data = get_query_analysis(api_key, db_name)
    
    query_stats = data.get('query_stats', {})
    total_queries = query_stats.get('total_queries', 0)
    qps = query_stats.get('queries_per_second', 0)
    
    print("ğŸ“‹ æŸ¥è¯¢å®¹é‡åˆ†æ")
    print(f"   å½“å‰ QPS: {qps:.1f}")
    print(f"   æ—¥å‡æŸ¥è¯¢é‡: {total_queries:,}")
    
    # åŸºäºå½“å‰è´Ÿè½½çš„å®¹é‡å»ºè®®
    if qps > 100:
        print("ğŸ’¡ å»ºè®®: å½“å‰è´Ÿè½½è¾ƒé«˜ï¼Œè€ƒè™‘è¯»å†™åˆ†ç¦»")
    elif qps > 50:
        print("ğŸ’¡ å»ºè®®: è´Ÿè½½é€‚ä¸­ï¼Œå®šæœŸç›‘æ§æ€§èƒ½")
    else:
        print("ğŸ’¡ å»ºè®®: è´Ÿè½½è¾ƒä½ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½")
```

## ç›¸å…³é“¾æ¥

- [ç›‘æ§ç»Ÿè®¡æ€»è§ˆ](../monitoring-stats/index.md)
- [æ•°æ®åº“ç»Ÿè®¡](../monitoring-stats/database-stats.md)
- [æ€§èƒ½ç»Ÿè®¡](../monitoring-stats/performance-stats.md)
- [API ä½¿ç”¨ç»Ÿè®¡](../monitoring-stats/api-usage-stats.md)