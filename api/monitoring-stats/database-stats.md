# æ•°æ®åº“ç»Ÿè®¡

## ç«¯ç‚¹ä¿¡æ¯

<ApiEndpoint method="GET" path="/stats/database"/>

è·å–æ‰€æœ‰æ•°æ®åº“çš„ç»Ÿè®¡æ¦‚è§ˆã€‚

<ApiEndpoint method="GET" path="/stats/database/{db_name}"/>

è·å–å•ä¸ªæ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚

## æƒé™è¦æ±‚
- `monitoring` æƒé™
- å¯¹ç›®æ ‡æ•°æ®åº“çš„ `read` æƒé™

## è¯·æ±‚

### æ‰€æœ‰æ•°æ®åº“ç»Ÿè®¡

#### è¯·æ±‚å¤´
| å¤´éƒ¨ | å€¼ | å¿…å¡« |
|------|-----|------|
| `Authorization` | `Bearer your_api_key` | æ˜¯ |

#### æŸ¥è¯¢å‚æ•°
| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|---------|------|
| `details` | boolean | å¦ | `false` | æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ |

### å•ä¸ªæ•°æ®åº“ç»Ÿè®¡

#### è¯·æ±‚å¤´
| å¤´éƒ¨ | å€¼ | å¿…å¡« |
|------|-----|------|
| `Authorization` | `Bearer your_api_key` | æ˜¯ |

#### è·¯å¾„å‚æ•°
| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `db_name` | string | æ˜¯ | æ•°æ®åº“åç§° |

## å“åº”

### æ‰€æœ‰æ•°æ®åº“ç»Ÿè®¡å“åº”

**çŠ¶æ€ç :** `200 OK`

```json
{
  "timestamp": "2024-01-20T15:30:00",
  "database_count": 5,
  "total_size": 5368709120,
  "total_tables": 45,
  "databases": [
    {
      "name": "users_db",
      "size_bytes": 1572864000,
      "table_count": 8,
      "row_count": 150000,
      "last_updated": "2024-01-20T14:25:00"
    },
    {
      "name": "orders_db",
      "size_bytes": 2147483648,
      "table_count": 12,
      "row_count": 500000,
      "last_updated": "2024-01-20T13:45:00"
    }
  ]
}
```

### å•ä¸ªæ•°æ®åº“è¯¦ç»†ç»Ÿè®¡å“åº”

**çŠ¶æ€ç :** `200 OK`

```json
{
  "name": "users_db",
  "size_bytes": 1572864000,
  "table_count": 8,
  "row_count": 150000,
  "last_updated": "2024-01-20T14:25:00",
  "size_human": "1.46 GB",
  "tables": [
    {
      "name": "users",
      "engine": "InnoDB",
      "row_count": 100000,
      "data_size": 805306368,
      "index_size": 134217728,
      "total_size": 939524096,
      "total_size_human": "896 MB",
      "create_time": "2024-01-15T10:30:00",
      "update_time": "2024-01-20T14:25:00"
    }
  ],
  "table_usage": [
    {
      "table_name": "users",
      "data_size": 805306368,
      "index_size": 134217728,
      "total_size": 939524096,
      "total_size_human": "896 MB",
      "row_count": 100000,
      "avg_row_length": 8053
    }
  ]
}
```

### å“åº”å­—æ®µè¯´æ˜

#### æ¦‚è§ˆç»Ÿè®¡
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `timestamp` | string | ç»Ÿè®¡æ—¶é—´æˆ³ |
| `database_count` | integer | æ•°æ®åº“æ•°é‡ |
| `total_size` | integer | æ€»æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰ |
| `total_tables` | integer | æ€»è¡¨æ•°é‡ |
| `databases` | array | æ•°æ®åº“åˆ—è¡¨ |

#### æ•°æ®åº“ä¿¡æ¯
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `name` | string | æ•°æ®åº“åç§° |
| `size_bytes` | integer | æ•°æ®åº“å¤§å°ï¼ˆå­—èŠ‚ï¼‰ |
| `table_count` | integer | è¡¨æ•°é‡ |
| `row_count` | integer | è¡Œæ•°ä¼°ç®— |
| `last_updated` | string | æœ€åæ›´æ–°æ—¶é—´ |

#### è¯¦ç»†ç»Ÿè®¡
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `size_human` | string | äººç±»å¯è¯»çš„æ•°æ®åº“å¤§å° |
| `tables` | array | è¡¨è¯¦ç»†ä¿¡æ¯åˆ—è¡¨ |
| `table_usage` | array | è¡¨ç©ºé—´ä½¿ç”¨æƒ…å†µ |

#### è¡¨ä¿¡æ¯
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `name` | string | è¡¨åç§° |
| `engine` | string | å­˜å‚¨å¼•æ“ |
| `row_count` | integer | è¡Œæ•° |
| `data_size` | integer | æ•°æ®å¤§å° |
| `index_size` | integer | ç´¢å¼•å¤§å° |
| `total_size` | integer | æ€»å¤§å° |
| `total_size_human` | string | äººç±»å¯è¯»çš„æ€»å¤§å° |
| `create_time` | string | åˆ›å»ºæ—¶é—´ |
| `update_time` | string | æ›´æ–°æ—¶é—´ |

## ä½¿ç”¨ç¤ºä¾‹

### cURL ç¤ºä¾‹

**è·å–æ‰€æœ‰æ•°æ®åº“ç»Ÿè®¡:**
```bash
curl -X GET "https://dbapi.muzilix.cn/stats/database?details=true" \
  -H "Authorization: Bearer your_api_key"
```

**è·å–å•ä¸ªæ•°æ®åº“ç»Ÿè®¡:**
```bash
curl -X GET https://dbapi.muzilix.cn/stats/database/users_db \
  -H "Authorization: Bearer your_api_key"
```

### Python ç¤ºä¾‹
```python
def get_database_stats(api_key, db_name=None):
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    if db_name:
        url = f"https://dbapi.muzilix.cn/stats/database/{db_name}"
    else:
        url = "https://dbapi.muzilix.cn/stats/database"
    
    headers = {"Authorization": f"Bearer {api_key}"}
    
    response = requests.get(url, headers=headers)
    return response.json()

def analyze_database_growth(api_key):
    """åˆ†ææ•°æ®åº“å¢é•¿è¶‹åŠ¿"""
    stats = get_database_stats(api_key)
    
    print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡æ¦‚è§ˆ")
    print(f"   æ•°æ®åº“æ•°é‡: {stats['database_count']}")
    print(f"   æ€»è¡¨æ•°é‡: {stats['total_tables']}")
    print(f"   æ€»æ•°æ®å¤§å°: {format_size(stats['total_size'])}")
    
    print("\nğŸ“ˆ å„æ•°æ®åº“è¯¦æƒ…:")
    for db in stats["databases"]:
        size_gb = db["size_bytes"] / (1024 ** 3)
        print(f"   ğŸ“ {db['name']}")
        print(f"      å¤§å°: {format_size(db['size_bytes'])}")
        print(f"      è¡¨æ•°: {db['table_count']}")
        print(f"      è¡Œæ•°: {db['row_count']:,}")

def monitor_large_tables(api_key, db_name, size_threshold_mb=100):
    """ç›‘æ§å¤§è¡¨ä½¿ç”¨æƒ…å†µ"""
    detailed_stats = get_database_stats(api_key, db_name)
    
    if "error" in detailed_stats:
        print(f"é”™è¯¯: {detailed_stats['error']}")
        return
    
    large_tables = []
    for table in detailed_stats["table_usage"]:
        size_mb = table["total_size"] / (1024 * 1024)
        if size_mb > size_threshold_mb:
            large_tables.append({
                "name": table["table_name"],
                "size_mb": size_mb,
                "rows": table["row_count"]
            })
    
    if large_tables:
        print(f"âš ï¸  {db_name} ä¸­çš„å¤§è¡¨ (> {size_threshold_mb}MB):")
        for table in sorted(large_tables, key=lambda x: x["size_mb"], reverse=True):
            print(f"   - {table['name']}: {table['size_mb']:.1f} MB, {table['rows']:,} è¡Œ")
    else:
        print(f"âœ… {db_name} ä¸­æ²¡æœ‰è¶…è¿‡ {size_threshold_mb}MB çš„å¤§è¡¨")

# ä½¿ç”¨ç¤ºä¾‹
# è·å–æ‰€æœ‰æ•°æ®åº“ç»Ÿè®¡
all_stats = get_database_stats("your_api_key")
analyze_database_growth("your_api_key")

# è·å–å•ä¸ªæ•°æ®åº“è¯¦ç»†ç»Ÿè®¡
db_stats = get_database_stats("your_api_key", "production_db")
monitor_large_tables("your_api_key", "production_db", 500)
```

## ç»Ÿè®¡æŒ‡æ ‡è¯´æ˜

### å¤§å°è®¡ç®—
æ•°æ®åº“æ€»å¤§å°åŒ…æ‹¬ï¼š
- è¡¨æ•°æ®å¤§å° (`data_length`)
- ç´¢å¼•å¤§å° (`index_length`)
- ä¸åŒ…æ‹¬ç³»ç»Ÿå¼€é”€

### è¡Œæ•°ä¼°ç®—
- åŸºäºå­˜å‚¨å¼•æ“çš„ç»Ÿè®¡ä¿¡æ¯
- å¯¹äº InnoDB æ˜¯ä¼°ç®—å€¼
- å®šæœŸæ›´æ–°ï¼Œå¯èƒ½ä¸æ˜¯å®æ—¶ç²¾ç¡®å€¼

### æ—¶é—´ä¿¡æ¯
- `last_updated`: æœ€åæ•°æ®æ›´æ–°æ—¶é—´
- `create_time`: è¡¨åˆ›å»ºæ—¶é—´
- `update_time`: æœ€åæ•°æ®ä¿®æ”¹æ—¶é—´

## ç›‘æ§åº”ç”¨

### å®¹é‡è§„åˆ’
```python
def capacity_planning_report(api_key):
    """å®¹é‡è§„åˆ’æŠ¥å‘Š"""
    stats = get_database_stats(api_key)
    
    total_size_gb = stats["total_size"] / (1024 ** 3)
    avg_db_size_gb = total_size_gb / stats["database_count"]
    
    print("ğŸ“‹ å®¹é‡è§„åˆ’æŠ¥å‘Š")
    print(f"   å½“å‰æ€»æ•°æ®é‡: {total_size_gb:.2f} GB")
    print(f"   å¹³å‡æ•°æ®åº“å¤§å°: {avg_db_size_gb:.2f} GB")
    print(f"   æ•°æ®åº“æ•°é‡: {stats['database_count']}")
    
    # å¢é•¿é¢„æµ‹ï¼ˆå‡è®¾æ¯æœˆå¢é•¿10%ï¼‰
    monthly_growth = 0.10
    projected_3mo = total_size_gb * (1 + monthly_growth) ** 3
    projected_12mo = total_size_gb * (1 + monthly_growth) ** 12
    
    print(f"\nğŸ“ˆ å¢é•¿é¢„æµ‹ (æ¯æœˆ {monthly_growth*100}%):")
    print(f"   3ä¸ªæœˆå: {projected_3mo:.2f} GB")
    print(f"   12ä¸ªæœˆå: {projected_12mo:.2f} GB")

# ä½¿ç”¨ç¤ºä¾‹
capacity_planning_report("your_api_key")
```

### å­˜å‚¨ä¼˜åŒ–å»ºè®®
```python
def storage_optimization_suggestions(api_key, db_name):
    """å­˜å‚¨ä¼˜åŒ–å»ºè®®"""
    stats = get_database_stats(api_key, db_name)
    
    if "error" in stats:
        return
    
    suggestions = []
    
    # åˆ†æç´¢å¼•å¤§å°å æ¯”
    for table in stats["table_usage"]:
        total_size = table["total_size"]
        index_size = table["index_size"]
        
        if total_size > 0:
            index_ratio = index_size / total_size
            
            if index_ratio > 0.5:
                suggestions.append(
                    f"è¡¨ '{table['table_name']}' ç´¢å¼•å æ¯”è¿‡é«˜ ({index_ratio:.1%})ï¼Œè€ƒè™‘ç´¢å¼•ä¼˜åŒ–"
                )
            
            if table["row_count"] > 1000000 and table["avg_row_length"] > 10000:
                suggestions.append(
                    f"è¡¨ '{table['table_name']}' è¡Œé•¿åº¦è¾ƒå¤§ï¼Œè€ƒè™‘æ•°æ®å½’æ¡£"
                )
    
    if suggestions:
        print(f"ğŸ’¡ {db_name} å­˜å‚¨ä¼˜åŒ–å»ºè®®:")
        for suggestion in suggestions:
            print(f"   - {suggestion}")
    else:
        print(f"âœ… {db_name} å­˜å‚¨ç»“æ„è‰¯å¥½")

# ä½¿ç”¨ç¤ºä¾‹
storage_optimization_suggestions("your_api_key", "analytics_db")
```

## ç›¸å…³é“¾æ¥

- [ç›‘æ§ç»Ÿè®¡æ€»è§ˆ](../monitoring-stats/index.md)
- [æ€§èƒ½ç»Ÿè®¡](performance-stats.md)
- [æŸ¥è¯¢åˆ†æ](query-analysis.md)
- [API ä½¿ç”¨ç»Ÿè®¡](api-usage-stats.md)